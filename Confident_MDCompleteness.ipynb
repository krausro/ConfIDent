{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3975010d-b74d-4efb-a180-f79f415c5f61",
   "metadata": {},
   "source": [
    "Check completeness of ConfIDent metadata:\n",
    "Collect metadata per DOI from API-response, calculate percenate of data missing per field, export to excel \n",
    "\n",
    "Metadata fields not included, since not applicable / never filled yet according to API query: \n",
    "sizes, formats, version (count 0), rightsList (count 1)\n",
    "\n",
    "Issues:\n",
    "Very repetetive (with variants)! Kapseln mÃ¶glich?\n",
    "\n",
    "\n",
    "\n",
    "To be done, if helpful:\n",
    "\n",
    "- Group entries from same dict in json in dicts here as well\n",
    "- Calculate percentages of all missing entries\n",
    "- Other form of display/export of results more useful?\n",
    "- Include type of description in collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a1803a4-971d-4316-8763-c04516baae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.close(\"all\")\n",
    "\n",
    "# Define the API URL\n",
    "URL = \"https://api.datacite.org/dois\"\n",
    "\n",
    "# Define the query parameters\n",
    "params = {\n",
    "    \"prefix\": \"10.25798\",\n",
    "    \"page[number]\": 1,\n",
    "    \"page[size]\": 1\n",
    "}\n",
    "\n",
    "# Initialize lists to store data, with repeatable entries as sublist per DOI\n",
    "\n",
    "# Of time stamps, use \"registered\" to reflect when the DOI was created with currently missing metadata \n",
    "# Assumption: updates add metadata rather than removing them\n",
    "registered = []\n",
    "dois = []\n",
    "# URL of landing page\n",
    "urls = []\n",
    "# Entry \"creator\" in DC-MD-Schema means \"hosting or initialising institution of event (series)\" in ConfIDent\n",
    "creators = []\n",
    "title = []\n",
    "publisher = []\n",
    "# Entry \"publication year\" in DC-MD-Schema means \"year the event took place\" in ConfIDent \n",
    "publication_year = []\n",
    "restype = []\n",
    "restypeg = []\n",
    "subjects = []\n",
    "contributors = []\n",
    "contributor_ids = []\n",
    "dates = []\n",
    "alternative_identifiers = []\n",
    "related_identifiers = []\n",
    "description = []\n",
    "# Stores geolocation property (type of geolocation given)\n",
    "geo_locations = []\n",
    "language = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to check if string already is in list of strings, independent of capitalization. Return None (no dublicate) if list is empty\n",
    "def check_for_dublicate (list, string):\n",
    "    try:\n",
    "        for j in range(len(list)):\n",
    "            if list[j].lower() != string.lower():\n",
    "                pass\n",
    "            else: \n",
    "                return True\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Fetch n pages of API-response and collect entries in \"data\" from single DOIs\n",
    "def fetch_data():    \n",
    "    response = requests.get(URL, params=params)\n",
    "    antwort = response.json()\n",
    "    \n",
    "    for item in antwort[\"data\"]:\n",
    "        attributes = item[\"attributes\"]\n",
    "        \n",
    "    # Collect non-repeatable entries\n",
    "        registered.append(attributes[\"registered\"])\n",
    "        dois.append(attributes[\"doi\"])\n",
    "    # If not given (though mandatory), the following values could be None-type\n",
    "    # If error occurs, cf . \"types\"\n",
    "        urls.append(attributes[\"url\"])\n",
    "        publication_year.append(attributes[\"publicationYear\"])\n",
    "        publisher.append(attributes[\"publisher\"])\n",
    "\n",
    "        if type(attributes[\"types\"][\"resourceType\"]) == str:\n",
    "            restypeg.append(attributes[\"types\"][\"resourceType\"])\n",
    "        elif attributes[\"types\"][\"resourceType\"] == None:\n",
    "            restypeg.append([])\n",
    "        \n",
    "        if type(attributes[\"types\"][\"resourceTypeGeneral\"]) == str:\n",
    "            restypeg.append(attributes[\"types\"][\"resourceTypeGeneral\"])\n",
    "        elif attributes[\"types\"][\"resourceTypeGeneral\"] == None:\n",
    "            restypeg.append([])\n",
    "                            \n",
    "        # Collect whole entry \"dates\" as list of possibly repeated entry \"date\" (with different date types) assigend to single DOI. \n",
    "        # Shoud reflect duration of event. Should return empty list if no entry was made.\n",
    "        dates.append(attributes[\"dates\"])\n",
    "        \n",
    "        # As of 20240426, language entry is either str (from controlled list) or None, \n",
    "        # but is planned to allow multiple entries for language\n",
    "        if type(attributes[\"language\"]) == str:\n",
    "            language.append(attributes[\"language\"])\n",
    "        elif attributes[\"language\"] == None:\n",
    "            language.append([])\n",
    "        else:\n",
    "            language.append(\"json has changed, someone needs to update the code\")\n",
    "         \n",
    "        # Collect multible entries from fields with (repeatable) subfields\n",
    "\n",
    "        eintrag_urheber_item = (attributes[\"creators\"])\n",
    "        urheber_item = []\n",
    "         # In case no creator is given: attributes[\"creators\"] == [], the following then is \"for i in range(0, 0)\" and not executed \n",
    "         # => no index error, list remains as empty list\n",
    "        for i in range(len(eintrag_urheber_item)):\n",
    "            urheber = eintrag_urheber_item[i][\"name\"]\n",
    "            if check_for_dublicate(urheber_item, urheber) == None:\n",
    "                urheber_item.append(urheber)\n",
    "        creators.append(urheber_item)\n",
    "\n",
    "        eintrag_titel_item = (attributes[\"titles\"])\n",
    "        titel_item = []\n",
    "        for i in range(len(eintrag_titel_item)):\n",
    "            titel = eintrag_titel_item[i][\"title\"]\n",
    "            if check_for_dublicate(titel_item, titel) == None:\n",
    "                titel_item.append(titel)\n",
    "        title.append(titel_item)\n",
    "        \n",
    "        eintrag_subjects_item = (attributes[\"subjects\"])\n",
    "        schlagwoerter_item = []\n",
    "        for i in range(len(eintrag_subjects_item)):\n",
    "            schlagwort = eintrag_subjects_item[i][\"subject\"]\n",
    "            if check_for_dublicate(schlagwoerter_item, schlagwort) == None:\n",
    "                # Capitalize freetext entries to avoid splitting of subject count due to inconsistent capitalization \n",
    "                if schlagwort[:4] != \"FOS:\":\n",
    "                    schlagwort = schlagwort.capitalize()\n",
    "                schlagwoerter_item.append(schlagwort)\n",
    "        subjects.append(schlagwoerter_item)\n",
    "        \n",
    "        eintrag_contributors_item = (attributes[\"contributors\"])\n",
    "        contributors_item = []\n",
    "        contributorids_item = []\n",
    "        for i in range(len(eintrag_contributors_item)):\n",
    "            contributor = eintrag_contributors_item[i][\"name\"]\n",
    "            contributors_item.append(contributor)\n",
    "            for j in range(len(eintrag_contributors_item[i][\"nameIdentifiers\"])):\n",
    "                contributorid = eintrag_contributors_item[i][\"nameIdentifiers\"][j].get(\"nameIdentifier\")\n",
    "                if contributorid:\n",
    "                    contributorids_item.append(contributorid)\n",
    "                else:\n",
    "                    contributorids_item.append([])\n",
    "        contributors.append(contributors_item)\n",
    "        contributor_ids.append(contributorids_item)\n",
    "\n",
    "        eintrag_relidents_item = (attributes[\"relatedIdentifiers\"])\n",
    "        relidents_item = []\n",
    "        for i in range(len(eintrag_relidents_item)):\n",
    "            relident = eintrag_relidents_item[i][\"relatedIdentifier\"]\n",
    "            relidents_item.append(relident)\n",
    "        related_identifiers.append(relidents_item)\n",
    "\n",
    "        eintrag_descriptions_item = (attributes[\"descriptions\"])\n",
    "        descriptions_item = []\n",
    "        for i in range(len(eintrag_descriptions_item)):\n",
    "            beschreibung = eintrag_descriptions_item[i][\"description\"]\n",
    "            # Fetch only first 100 characters of description\n",
    "            descriptions_item.append(beschreibung[:100])\n",
    "        description.append(descriptions_item)\n",
    "\n",
    "        eintrag_geolocations_item = (attributes[\"geoLocations\"])\n",
    "        geolocations_item = []\n",
    "        for i in range(len(eintrag_geolocations_item)):\n",
    "            ortsangabe = list(eintrag_geolocations_item[i].keys())\n",
    "            geolocations_item.append(ortsangabe)\n",
    "        geo_locations.append(geolocations_item)\n",
    "\n",
    "        eintrag_altidents_item = (attributes[\"identifiers\"])\n",
    "        altidents_item = []\n",
    "        for i in range(len(eintrag_altidents_item)):\n",
    "            altid = eintrag_altidents_item[i][\"identifier\"]\n",
    "            altidents_item.append(altid)\n",
    "        alternative_identifiers.append(altidents_item)\n",
    "        \n",
    "    return antwort[\"links\"].get(\"next\")\n",
    "\n",
    "\n",
    "while params[\"page[number]\"] < 2: \n",
    "#while True:\n",
    "    next_page = fetch_data()\n",
    "    # Check if there is a next page to fetch, if yes, count up page number of query\n",
    "    if next_page == None:\n",
    "        break\n",
    "    else: \n",
    "        params[\"page[number]\"] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ea34ec2-889c-4372-87aa-8af53d50fc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of metadata sets with specific entry missing\n",
    "# for all entries listed except \"DOI\" and \"registered\" (always 100%)\n",
    "\n",
    "number_dois = len(dois)\n",
    "md_fields = [registered,\n",
    "             dois,\n",
    "             urls,\n",
    "             creators,\n",
    "             title,\n",
    "             publisher,\n",
    "             publication_year,\n",
    "             restype,\n",
    "             restypeg,\n",
    "             subjects,\n",
    "             contributors,\n",
    "             contributor_ids,\n",
    "             dates,\n",
    "             alternative_identifiers,\n",
    "             related_identifiers,\n",
    "             description,\n",
    "             geo_locations,\n",
    "             language]\n",
    "\n",
    "for field in md_fields: \n",
    "    no_entry = 0\n",
    "    for i in range(len(field)):\n",
    "        if field[i] == []:\n",
    "            no_entry +=1\n",
    "    perc_no_entry = round(((no_entry / number_dois) * 100), 2)\n",
    "    field.insert(0, f\"Percentage missing: {perc_no_entry}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a31d2314-95f9-43ed-812e-061dee8abccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the extracted data per DOI\n",
    "\n",
    "# Untrancate list - does not work with broad columns\n",
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 4000)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Registration Date\" : registered,\n",
    "    \"DOI\": dois,\n",
    "    \"URL\": urls,\n",
    "    \"Institution\": creators,\n",
    "    \"Title\": title,\n",
    "    \"Publisher\": publisher,\n",
    "    \"Year of Event\": publication_year,\n",
    "    \"Other Dates of Event\": dates,\n",
    "    \"Resource Type General\": restypeg,\n",
    "    \"Resource Type\": restype,\n",
    "    \"Subject\": subjects,\n",
    "    \"Contributors\": contributors,\n",
    "    \"Contributor IDs\": contributor_ids,\n",
    "    \"Related Identifiers\": related_identifiers,\n",
    "    \"Description\": description,\n",
    "    \"Geo Location\": geo_locations, \n",
    "    \"Language\": language,\n",
    "    \"Alternative ID\": alternative_identifiers\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc5e89f9-fdf7-41ba-8d48-bde358208c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fertig!\n"
     ]
    }
   ],
   "source": [
    "#print(df)\n",
    "# Export DataFrame to Excel file\n",
    "df.to_excel(\"ConfIDent_MD-completeness.xlsx\")\n",
    "print(\"fertig!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aec674-0d86-4920-9780-f2f56858acec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
